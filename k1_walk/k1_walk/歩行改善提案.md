# K1ロボット歩行改善提案

## 現在の問題

1. **足を地面に引きずったまま移動する**
2. **足の動きが左右対称にならない**

---

## 問題の原因と改善策

### 1. 足を地面に引きずる問題

#### 原因分析

**報酬関数の重み不足** (`rough_env_cfg.py:291-345`)
- `foot_symmetry_height` (足の高さ制御): weight=0.5 と非常に低い
- `feet_air_time` (足の空中時間): weight=1.6
- 足を上げるインセンティブが、他の報酬（速度追従、姿勢維持など）に対して相対的に弱い

**アクチュエーター設定の問題** (`rough_env_cfg.py:108-121`)
```python
".*_Ankle_.*": {
    stiffness: 50.0,   # 他の関節(200.0)に比べて低い
    damping: 1.0       # 他の関節(5.0)に比べて低い
}
```
- 足首が柔らかすぎて、地面を蹴る力が不足
- 足を持ち上げる際のコントロールが不安定

#### 改善策

**A. 報酬の重み調整**

報酬の重みは、**カリキュラム学習で自動的に調整**されるため、基本的には初期値（0.5と1.6）のままで問題ありません。カリキュラム学習が訓練の進行に応じて自動的に重みを増やしていきます。

ただし、以下のパラメータは直接調整することを推奨:

```python
# rough_env_cfg.py:337-345
feet_air_time = RewTerm(
    func=mdp.feet_air_time_positive_biped,
    weight=1.6,  # カリキュラムで調整されるので初期値はそのまま
    params={
        "threshold": 0.7,  # → 0.5 に下げる（より早く報酬を与える）
        # より小さい空中時間でも報酬を与えることで、学習初期から足上げを促進
    },
)

# rough_env_cfg.py:291-294
foot_symmetry_height = RewTerm(
    func=mdp.foot_ref_height,
    weight=0.5,  # カリキュラムで調整されるので初期値はそのまま
    params={
        "target_height": 0.18,  # 0.15~0.20で調整可能
        "frequency": 1.5,       # → 2.0 に変更（clock_phaseと統一）
        "sigma": 0.17,
    },
)
```

**重要**: カリキュラム学習を使う場合、初期weightを直接変更する必要はありません。カリキュラムの**最終目標値**（下記セクションC参照）を適切に設定することが重要です。

**優先度: 中** - カリキュラム学習の調整（セクションC）の方が優先度高

**B. アクチュエーター設定の強化** (`rough_env_cfg.py:108-121`)

```python
".*_Ankle_.*": {
    stiffness: 100.0,  # 50.0 → 100.0~150.0
    damping: 2.0,      # 1.0 → 2.0~3.0
}
```

**優先度: 中** - 物理的な動作品質の向上

**C. カリキュラム学習の調整** (`rough_env_cfg.py:452-453`) ⭐重要⭐

現在のカリキュラム学習のアプローチは**正しい判断**です。しかし、**最終目標値が小さすぎる**ことが問題です。

#### 現在の設定の問題点

```python
# rough_env_cfg.py:452-453
foot_height_weights = CurrTerm(
    func=mdp.modify_reward_weight,
    params={
        "term_name": "foot_symmetry_height",
        "weight": 1.0,      # 最終値: 0.5 → 1.0 (2倍)
        "num_steps": 10000  # 訓練の40%で到達
    }
)
```

**問題**:
- 最終値1.0では足上げのインセンティブがまだ不足
- 10000ステップは総訓練25000ステップの40%で早すぎる
- 残り60%の訓練時間で「足引きずりパターン」が定着してしまう

#### なぜカリキュラム学習が必要か

**カリキュラムなし（いきなり高い重み）の問題**:
1. まだ姿勢制御ができていない状態で無理に足を上げる
2. 転倒が頻発し、有益な経験データが得られない
3. 学習が停滞または発散する

**カリキュラムあり（段階的に増加）の利点**:
1. 初期: 基本的な移動と姿勢維持を学習（足引きずりOK）
2. 中期: 徐々に足を上げる動作を学習
3. 後期: しっかりとした足上げで歩行を完成

#### 推奨する改善案

**オプション1: 最終目標値を引き上げる（推奨）**

```python
@configclass
class CurriculumCfg:
    """Curriculum terms for the MDP."""
    terrain_levels = CurrTerm(func=mdp.terrain_levels_vel)

    # 足の高さ報酬を段階的に強化
    foot_height_weights = CurrTerm(
        func=mdp.modify_reward_weight,
        params={
            "term_name": "foot_symmetry_height",
            "weight": 4.5,      # 0.5 → 4.5 (9倍に増加)
            "num_steps": 15000  # 総訓練の60%でゆっくり到達
        }
    )

    # 空中時間報酬も併せて強化（推奨）
    air_time_weights = CurrTerm(
        func=mdp.modify_reward_weight,
        params={
            "term_name": "feet_air_time",
            "weight": 5.0,      # 1.6 → 5.0
            "num_steps": 15000
        }
    )
```

**段階的な学習曲線**:
- **0-5000ステップ** (0-20%): weight 0.5-1.5
  - 目標: 倒れずに移動する基礎を確立
  - 足の引きずりは許容

- **5000-10000ステップ** (20-40%): weight 1.5-2.5
  - 目標: 少しずつ足を上げ始める
  - 姿勢制御と足上げの両立を学習

- **10000-15000ステップ** (40-60%): weight 2.5-4.5
  - 目標: 明確な足上げ動作を獲得
  - 周期的な歩行パターンの確立

- **15000-25000ステップ** (60-100%): weight 4.5固定
  - 目標: 動作の洗練とロバスト性向上
  - 高い報酬重みの下で最適化

**オプション2: 2段階カリキュラム**（より細かい制御が必要な場合）

```python
# 第1段階: 基礎的な足上げ (0-8000ステップ)
foot_height_weights_phase1 = CurrTerm(
    func=mdp.modify_reward_weight,
    params={
        "term_name": "foot_symmetry_height",
        "weight": 2.5,
        "num_steps": 8000
    }
)

# 第2段階: より高い足上げ (8000-18000ステップ)
foot_height_weights_phase2 = CurrTerm(
    func=mdp.modify_reward_weight,
    params={
        "term_name": "foot_symmetry_height",
        "weight": 5.0,
        "num_steps": 18000
    }
)
```

**優先度: 最高** - カリキュラムの最終目標値を上げることが最重要

---

### 2. 左右非対称な動きの問題

#### 原因分析

**周期的動作の学習不足**
- `foot_ref_height` (`rewards.py:206-232`) で左右の足に位相差(0.5)を与えているが、weight=0.5と弱い
- 結果として、左右独立に動く方が他の報酬を稼ぎやすい状態

**対称性制約が部分的** (`rewards.py:234-301`)
- `joint_reqularization_potential`はRoll/Yaw関節のみ左右対称性を強制
- Pitch関節（歩行に最も重要）は意図的に非対称を許可している
- これは正常な歩行には必要だが、バランスが悪いと異常な動作になる

**接地パターンの制約不足**
- 「片足が地面、もう片方が空中」という歩行の基本パターンを強制する報酬が弱い
- `feet_air_time_positive_biped`はこれを促しているが、weight=1.6では不十分

#### 改善策

**A. 周期報酬の強化** (`rough_env_cfg.py:291-294`)

```python
foot_symmetry_height = RewTerm(
    func=mdp.foot_ref_height,
    weight=0.5,  # カリキュラム学習で自動調整（最終的に4.5へ）
    params={
        "target_height": 0.18,  # 0.15~0.20で調整
        "frequency": 1.5,       # → 2.0 に変更（clock_phaseと統一）
        "sigma": 0.17,
    },
)
```

**重要**: weightの調整は**カリキュラム学習で自動的に行われる**ため、ここでは主に`frequency`パラメータの調整に焦点を当てます。左右交互の動きを促進するには、観測(`clock_phase`)と報酬(`foot_ref_height`)の周波数を統一することが重要です。

**優先度: 高** - frequencyの統一が左右対称性に効果的

**B. clock_phaseの周波数調整** (`rough_env_cfg.py:253`)

```python
clock_phase = ObsTerm(
    func=mdp.clock_phase,
    params={"frequency": 1.5}  # → 2.0~2.5 に統一
)
```

`foot_ref_height`のfrequencyと揃えることで、観測と報酬が一貫する

**優先度: 高**

**C. 新しい報酬の追加案**

以下の報酬を追加することを検討:

```python
# 片足接地の報酬（単脚支持期を促進）
single_support_reward = RewTerm(
    func=mdp.single_support_phase,  # 新規実装が必要
    weight=2.0,
    params={
        "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_foot_link"),
    },
)

# 足の前後位置の対称性（左右の足が前後に開くことを促す）
feet_stride_symmetry = RewTerm(
    func=mdp.feet_stride_symmetry,  # 新規実装が必要
    weight=1.5,
)
```

**優先度: 低** - まずは既存の報酬調整を優先

**D. 正則化の強化** (`rough_env_cfg.py:287-289`)

```python
joint_reqularization_potential = RewTerm(
    func=mdp.joint_reqularization_potential,
    weight=1.0,  # → 2.0~3.0 に増やす
    params={"sigma": 0.4}
)
```

**優先度: 中**

---

### 3. その他の調整

#### A. トルクペナルティの調整

現在の設定 (`rough_env_cfg.py:301-305`):
```python
joint_torque = RewTerm(
    func=mdp.joint_torques_l2,
    weight=-5e-5,  # 非常に小さい
)
```

**問題**: トルクペナルティが弱すぎて、非効率な動作（足を引きずる）を抑制できていない

**改善案**: `-5e-5` → `-1e-4 ~ -5e-4` に増やす

**優先度: 中**

#### B. action_rateペナルティの調整

```python
action_rate_l2 = RewTerm(
    func=mdp.action_rate_l2,
    weight=-1e-3  # → -5e-4 に緩める
)
```

アクションの変化を過度に抑制すると、動的な足上げ動作が妨げられる可能性

**優先度: 低**

#### C. エピソード長の調整

```python
# rough_env_cfg.py:471
self.episode_length_s = 20.0  # → 30.0~40.0 に延ばす
```

より長いエピソードで、周期的な歩行パターンを学習する時間を確保

**優先度: 低**

---

## 推奨する実施手順

カリキュラム学習を前提とした段階的なアプローチを推奨します。

### 最優先: カリキュラム学習の設定変更

まず、カリキュラム学習の最終目標値を引き上げます。これが**最も重要**です。

**rough_env_cfg.py:449-453**を以下のように変更:

```python
@configclass
class CurriculumCfg:
    """Curriculum terms for the MDP."""
    terrain_levels = CurrTerm(func=mdp.terrain_levels_vel)

    # 足の高さ報酬のカリキュラム
    foot_height_weights = CurrTerm(
        func=mdp.modify_reward_weight,
        params={
            "term_name": "foot_symmetry_height",
            "weight": 4.5,      # 1.0 → 4.5 に変更
            "num_steps": 15000  # 10000 → 15000 に変更
        }
    )

    # 空中時間報酬のカリキュラム（新規追加）
    air_time_weights = CurrTerm(
        func=mdp.modify_reward_weight,
        params={
            "term_name": "feet_air_time",
            "weight": 5.0,      # 1.6 → 5.0
            "num_steps": 15000
        }
    )
```

**期待される学習プロセス**:
- **0-5000ステップ**: 姿勢維持と基本移動（足引きずりOK）
- **5000-10000ステップ**: 足を少しずつ上げ始める
- **10000-15000ステップ**: 明確な足上げ動作を獲得
- **15000-25000ステップ**: 動作の洗練

### Phase 1: 物理パラメータの調整

カリキュラム設定変更後、以下を調整:

1. **足首のstiffness** (`rough_env_cfg.py:113`): 50.0 → **100.0**
2. **足首のdamping** (`rough_env_cfg.py:120`): 1.0 → **2.0**
3. **`feet_air_time`のthreshold** (`rough_env_cfg.py:343`): 0.7 → **0.5**

**期待される効果**: より力強い足上げ動作と安定した着地

### Phase 2: 周期性と左右対称性の向上

Phase 1で足が上がるようになったら、以下を調整:

1. **`clock_phase`のfrequency** (`rough_env_cfg.py:253`): 1.5 → **2.0**
2. **`foot_ref_height`のfrequency** (`rough_env_cfg.py:293`): 1.5 → **2.0**（統一）
3. **`joint_reqularization_potential`のweight** (`rough_env_cfg.py:287`): 1.0 → **2.0**

**期待される効果**: 左右の足が交互に、より規則的に動くようになる

### Phase 3: 動作の洗練（オプション）

さらに改善が必要な場合:

1. **`joint_torque`のweight** (`rough_env_cfg.py:302`): -5e-5 → **-2e-4**
2. **足首のstiffness**: 100.0 → **120.0~150.0**（必要に応じて）
3. **`foot_ref_height`のtarget_height** (`rough_env_cfg.py:293`): 0.18 → **0.15~0.20**で微調整

**期待される効果**: より効率的でスムーズな歩行

### 重要な注意事項

**カリキュラム学習を活用する場合**:
- 上記のPhaseは「パラメータ変更のフェーズ」であり、「学習のフェーズ」ではありません
- カリキュラムが自動的に学習を段階的に進めてくれます
- 一度に全ての変更を適用し、25000ステップ訓練して結果を見る方が効率的
- 段階的な効果確認は、TensorBoardで報酬の推移を見て判断

---

## 実装上の注意点

### 1. ハイパーパラメータの調整方法

**段階的に変更する（カリキュラム学習を使う場合）**
- カリキュラム学習を使う場合、推奨する変更を**一度に全て適用**してから訓練開始
- カリキュラムが自動的に段階的な学習を進めてくれる
- TensorBoardで報酬の推移を監視し、学習の各段階を確認

**段階的に変更する（カリキュラム学習を使わない場合）**
- 一度に複数のパラメータを大きく変えない
- Phase 1の効果を確認してからPhase 2へ進む
- 各変更後、少なくとも5000~10000 timestepsは訓練して効果を確認

**学習のリセット**
- **カリキュラム学習の最終目標値を変更した場合**: ゼロから学習し直すことを推奨
  - カリキュラムの進行状況と既存モデルの不一致を避けるため
- **パラメータの微調整（threshold、frequencyなど）**: 既存チェックポイントから継続可能
- 判断に迷う場合は、ゼロからの方が安全

### 2. 評価方法

**カリキュラム学習の進行状況を監視**:
- TensorBoardで`foot_symmetry_height`と`feet_air_time`の報酬重みの推移を確認
- 15000ステップで目標値（4.5と5.0）に到達していることを確認
- 各学習段階（0-5000、5000-10000、10000-15000）で報酬がどう変化するかを観察

**定量的指標**:
- 平均足上げ高さ: `foot_right_height`, `foot_left_height`の時系列平均
- 空中時間の割合: `contact_sensor.data.current_air_time`の統計
- 報酬の内訳: TensorBoard等で各報酬項目を個別に監視
- **特に重要**: `foot_symmetry_height`と`feet_air_time`の報酬値が訓練中に増加していることを確認

**定性的指標**:
- 可視化で動作を確認（`--headless`を外して実行）
- 足を引きずっていないか
- 左右交互に動いているか
- 動作が段階的に改善されているか（5000ステップごとに確認）

### 3. トラブルシューティング

**カリキュラム学習が機能していない（報酬重みが変化していない）**:
- `modify_reward_weight`関数が正しく実装されているか確認
- TensorBoardで報酬重みのグラフを確認
- カリキュラムのterm_nameが報酬定義のキーと一致しているか確認

**15000ステップを過ぎても足を引きずっている**:
- カリキュラムの最終目標値をさらに高くする（4.5 → 6.0）
- `feet_air_time`のthresholdをさらに下げる（0.5 → 0.3）
- 足首のstiffnessを上げる（100.0 → 150.0）

**初期段階（0-5000ステップ）で頻繁に転倒する**:
- カリキュラムの進行を遅くする（num_steps: 15000 → 20000）
- 初期段階の報酬重みが適切か確認（0.5と1.6のままで良い）
- `orientation_potential`や`height_potential`の重みを上げて姿勢安定性を優先

**足を上げすぎる場合**:
- `foot_symmetry_height`のtarget_heightを下げる（0.18 → 0.15）
- カリキュラムの最終目標値を下げる（4.5 → 3.5）

**動作が不安定になる場合**:
- カリキュラムの進行を緩やかにする（num_steps: 15000 → 20000）
- `action_rate_l2`のペナルティを強くする（-1e-3 → -2e-3）
- 足首のstiffnessを少し下げる（100.0 → 80.0）

**学習が収束しない場合**:
- learning_rateを下げる（1e-3 → 5e-4）
- rollouts数を増やす（24 → 32）
- カリキュラムの最終目標値が高すぎないか確認（4.5 → 3.5に下げてみる）

---

## 参考: 関連ファイルと行番号

### 環境設定
- **メイン環境設定**: `source/k1_walk/k1_walk/tasks/manager_based/k1_walk/rough_env_cfg.py`
  - 報酬定義: 265-355行
  - アクチュエーター設定: 89-142行
  - 観測定義: 228-261行
  - **カリキュラム学習設定**: 449-453行（⭐最重要⭐）

### 報酬関数実装
- **報酬関数**: `source/k1_walk/k1_walk/tasks/manager_based/k1_walk/mdp/rewards.py`
  - `feet_air_time_positive_biped`: 48-67行
  - `foot_ref_height`: 206-232行
  - `joint_reqularization_potential`: 234-301行
  - `feet_parallel_to_ground`: 365-404行

### 観測実装
- **観測関数**: `source/k1_walk/k1_walk/tasks/manager_based/k1_walk/mdp/observations.py`
  - `clock_phase`: 63-95行
  - `feet_contact`: 35-59行

### トレーニングスクリプト
- **訓練実行**: `scripts/skrl/train.sh`
- **エージェント設定**: `source/k1_walk/k1_walk/tasks/manager_based/k1_walk/agents/skrl_ppo_cfg.yaml`

---

## まとめ

### 最も重要な改善: カリキュラム学習の最終目標値を引き上げる

現在のカリキュラム学習のアプローチは正しいですが、**最終目標値が1.0では不十分**です。

**推奨する変更**:
- `foot_symmetry_height`の最終weight: 1.0 → **4.5**
- `feet_air_time`の最終weight: 1.6 → **5.0**
- 到達ステップ数: 10000 → **15000**

これにより、カリキュラムが以下のように機能します:
1. **初期段階** (0-5000): 基本的な移動と姿勢維持（足引きずりOK）
2. **中期段階** (5000-10000): 徐々に足を上げ始める
3. **後期段階** (10000-15000): 明確な足上げ動作を獲得
4. **洗練段階** (15000-25000): 高い報酬重みの下で最適化

### その他の重要な調整

- **足首のstiffness**: 50.0 → 100.0（より力強い蹴り出し）
- **周波数の統一**: `clock_phase`と`foot_ref_height`を2.0で統一（左右交互動作の促進）
- **観測と報酬の一貫性**: 周期的な動作パターンを観測と報酬で一貫して促す

### 実施のポイント

カリキュラム学習を使う場合:
- **一度に全ての変更を適用**して、25000ステップ訓練
- カリキュラムが自動的に段階的な学習を進めてくれる
- TensorBoardで報酬の推移を監視し、学習の進行を確認
- 「足引きずり」→「足上げ」の移行が15000ステップ前後で起こることを期待
